{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import libraries and load data from database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sinde\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sinde\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import nltk\n",
    "nltk.download(['punkt', 'wordnet'])\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine,inspect\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import hamming_loss, confusion_matrix, classification_report, precision_recall_fscore_support, accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move to datasets folder\n",
    "original_directory = os.getcwd()\n",
    "dataset_directory = './dataset'\n",
    "os.chdir(dataset_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['messages']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Look for the tables name in the SQL database\n",
    "engine = create_engine('sqlite:///DisasterResponse.db')\n",
    "\n",
    "# Create an inspector\n",
    "inspector = inspect(engine)\n",
    "\n",
    "# Get the list of table names\n",
    "table_names = inspector.get_table_names()\n",
    "\n",
    "table_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///DisasterResponse.db')\n",
    "df = pd.read_sql(\"SELECT * FROM messages\", engine)\n",
    "X = df['message']\n",
    "Y = df.drop(['id','message','original','genre'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Weather update - a cold front from Cuba that c...\n",
       "1              Is the Hurricane over or is it not over\n",
       "2                      Looking for someone but no name\n",
       "3    UN reports Leogane 80-90 destroyed. Only Hospi...\n",
       "4    says: west side of Haiti, rest of the country ...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>related</th>\n",
       "      <th>request</th>\n",
       "      <th>offer</th>\n",
       "      <th>aid_related</th>\n",
       "      <th>medical_help</th>\n",
       "      <th>medical_products</th>\n",
       "      <th>search_and_rescue</th>\n",
       "      <th>security</th>\n",
       "      <th>military</th>\n",
       "      <th>child_alone</th>\n",
       "      <th>...</th>\n",
       "      <th>aid_centers</th>\n",
       "      <th>other_infrastructure</th>\n",
       "      <th>weather_related</th>\n",
       "      <th>floods</th>\n",
       "      <th>storm</th>\n",
       "      <th>fire</th>\n",
       "      <th>earthquake</th>\n",
       "      <th>cold</th>\n",
       "      <th>other_weather</th>\n",
       "      <th>direct_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   related  request  offer  aid_related  medical_help  medical_products  \\\n",
       "0        1        0      0            0             0                 0   \n",
       "1        1        0      0            1             0                 0   \n",
       "2        1        0      0            0             0                 0   \n",
       "3        1        1      0            1             0                 1   \n",
       "4        1        0      0            0             0                 0   \n",
       "\n",
       "   search_and_rescue  security  military  child_alone  ...  aid_centers  \\\n",
       "0                  0         0         0            0  ...            0   \n",
       "1                  0         0         0            0  ...            0   \n",
       "2                  0         0         0            0  ...            0   \n",
       "3                  0         0         0            0  ...            0   \n",
       "4                  0         0         0            0  ...            0   \n",
       "\n",
       "   other_infrastructure  weather_related  floods  storm  fire  earthquake  \\\n",
       "0                     0                0       0      0     0           0   \n",
       "1                     0                1       0      1     0           0   \n",
       "2                     0                0       0      0     0           0   \n",
       "3                     0                0       0      0     0           0   \n",
       "4                     0                0       0      0     0           0   \n",
       "\n",
       "   cold  other_weather  direct_report  \n",
       "0     0              0              0  \n",
       "1     0              0              0  \n",
       "2     0              0              0  \n",
       "3     0              0              0  \n",
       "4     0              0              0  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    clean_tokens = []\n",
    "    for tok in tokens:\n",
    "        clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n",
    "        clean_tokens.append(clean_tok)\n",
    "\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "I´ve used the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect',  CountVectorizer(tokenizer=tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf',   MultiOutputClassifier(RandomForestClassifier()))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(X_train, Y_train)\n",
    "\n",
    "# Predict on test data\n",
    "Y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.9452\n",
      "Macro Average Precision: 0.9364\n",
      "Macro Average Recall: 0.9452\n",
      "Macro Average F1 Score: 0.9307\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store the precision, recall, and f1-score for each label\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "f1_list = []\n",
    "\n",
    "# Calculate precision, recall, and f1-score for each label\n",
    "for i, column in enumerate(Y.columns):\n",
    "    precision = precision_score(Y_test[column], Y_pred[:, i], average='weighted', zero_division=0)\n",
    "    recall = recall_score(Y_test[column], Y_pred[:, i], average='weighted', zero_division=0)\n",
    "    f1 = f1_score(Y_test[column], Y_pred[:, i], average='weighted', zero_division=0)\n",
    "    \n",
    "    precision_list.append(precision)\n",
    "    recall_list.append(recall)\n",
    "    f1_list.append(f1)\n",
    "\n",
    "# Compute macro averages\n",
    "precision_macro = np.mean(precision_list)\n",
    "recall_macro = np.mean(recall_list)\n",
    "f1_macro = np.mean(f1_list)\n",
    "\n",
    "# Overall metrics\n",
    "overall_accuracy = (Y_pred == Y_test).mean().mean()\n",
    "\n",
    "print(f'Overall Accuracy: {overall_accuracy:.4f}')\n",
    "print(f'Macro Average Precision: {precision_macro:.4f}')\n",
    "print(f'Macro Average Recall: {recall_macro:.4f}')\n",
    "print(f'Macro Average F1 Score: {f1_macro:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n",
      "[CV 1/2] END clf__estimator__min_samples_split=2, clf__estimator__n_estimators=50;, score=0.234 total time=  50.9s\n",
      "[CV 2/2] END clf__estimator__min_samples_split=2, clf__estimator__n_estimators=50;, score=0.233 total time=  57.7s\n",
      "[CV 1/2] END clf__estimator__min_samples_split=2, clf__estimator__n_estimators=100;, score=0.233 total time= 1.9min\n",
      "[CV 2/2] END clf__estimator__min_samples_split=2, clf__estimator__n_estimators=100;, score=0.232 total time= 2.0min\n",
      "[CV 1/2] END clf__estimator__min_samples_split=2, clf__estimator__n_estimators=200;, score=0.237 total time= 3.7min\n",
      "[CV 2/2] END clf__estimator__min_samples_split=2, clf__estimator__n_estimators=200;, score=0.234 total time=12.5min\n",
      "[CV 1/2] END clf__estimator__min_samples_split=4, clf__estimator__n_estimators=50;, score=0.230 total time=  54.0s\n",
      "[CV 2/2] END clf__estimator__min_samples_split=4, clf__estimator__n_estimators=50;, score=0.229 total time=  57.2s\n",
      "[CV 1/2] END clf__estimator__min_samples_split=4, clf__estimator__n_estimators=100;, score=0.232 total time= 1.8min\n",
      "[CV 2/2] END clf__estimator__min_samples_split=4, clf__estimator__n_estimators=100;, score=0.233 total time= 1.8min\n",
      "[CV 1/2] END clf__estimator__min_samples_split=4, clf__estimator__n_estimators=200;, score=0.234 total time= 3.5min\n",
      "[CV 2/2] END clf__estimator__min_samples_split=4, clf__estimator__n_estimators=200;, score=0.232 total time= 3.7min\n",
      "[CV 1/2] END clf__estimator__min_samples_split=6, clf__estimator__n_estimators=50;, score=0.229 total time=  55.1s\n",
      "[CV 2/2] END clf__estimator__min_samples_split=6, clf__estimator__n_estimators=50;, score=0.225 total time=  55.9s\n",
      "[CV 1/2] END clf__estimator__min_samples_split=6, clf__estimator__n_estimators=100;, score=0.229 total time= 1.8min\n",
      "[CV 2/2] END clf__estimator__min_samples_split=6, clf__estimator__n_estimators=100;, score=0.230 total time= 1.8min\n",
      "[CV 1/2] END clf__estimator__min_samples_split=6, clf__estimator__n_estimators=200;, score=0.234 total time= 3.6min\n",
      "[CV 2/2] END clf__estimator__min_samples_split=6, clf__estimator__n_estimators=200;, score=0.229 total time= 4.4min\n",
      "Initial Best Parameters: {'clf__estimator__min_samples_split': 2, 'clf__estimator__n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "parameters_initial = {\n",
    "    'clf__estimator__n_estimators': [50, 100, 200],\n",
    "    'clf__estimator__min_samples_split': [2, 4, 6]\n",
    "}\n",
    "\n",
    "cv = GridSearchCV(pipeline, param_grid=parameters_initial, cv=2, verbose=3)\n",
    "cv.fit(X_train, Y_train)\n",
    "\n",
    "print(\"Initial Best Parameters:\", cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Considering performance (time and criteria 'gini index' for RandomForestClassifier Machine Learning Model)\n",
    "pipeline.set_params(clf__estimator__n_estimators=50,clf__estimator__min_samples_split=2)\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(X_train, Y_train)\n",
    "\n",
    "# Predict on test data\n",
    "Y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Initialize lists to store the precision, recall, and f1-score for each label\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "f1_list = []\n",
    "\n",
    "# Calculate precision, recall, and f1-score for each label\n",
    "for i, column in enumerate(Y.columns):\n",
    "    precision = precision_score(Y_test[column], Y_pred[:, i], average='weighted', zero_division=0)\n",
    "    recall = recall_score(Y_test[column], Y_pred[:, i], average='weighted', zero_division=0)\n",
    "    f1 = f1_score(Y_test[column], Y_pred[:, i], average='weighted', zero_division=0)\n",
    "    \n",
    "    precision_list.append(precision)\n",
    "    recall_list.append(recall)\n",
    "    f1_list.append(f1)\n",
    "\n",
    "# Compute macro averages\n",
    "precision_macro = np.mean(precision_list)\n",
    "recall_macro = np.mean(recall_list)\n",
    "f1_macro = np.mean(f1_list)\n",
    "\n",
    "# Overall metrics\n",
    "overall_accuracy = (Y_pred == Y_test).mean().mean()\n",
    "\n",
    "print(f'Overall Accuracy: {overall_accuracy:.4f}')\n",
    "print(f'Macro Average Precision: {precision_macro:.4f}')\n",
    "print(f'Macro Average Recall: {recall_macro:.4f}')\n",
    "print(f'Macro Average F1 Score: {f1_macro:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model - Text the model effectiveness using a new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datsets\n",
    "twitter = pd.read_csv('Twitter-sentiment-self-drive-DFE.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two places I'd invest all my money if I could: 3D printing and Self-driving cars!!!\n",
      "Awesome! Google driverless cars will help the blind travel more often; https://t.co/QWuXR0FrBpv\n",
      "If Google maps can't keep up with road construction, how am I supposed to trust a driverless car to get around here?\n",
      "Autonomous cars seem way overhyped given the technology challenges; pilotless planes seem much more doable and needed.\n",
      "Just saw Google self-driving car on I-34. It was painted green and blue.\n",
      "Will driverless cars eventually replace taxi drivers in cities?\n",
      "Chicago metro expected to be fully autonomous by 2020\n",
      "I love the infotainment system in my new car. This thing can almost drive itself.\n",
      "Autonomous vehicles could reduce traffic fatalities by 90%...I'm in!\n",
      "Driverless cars are not worth the risk.  Don't want to be on the highway when the server crashes #SadMacFace #BlueScreenofDeath\n"
     ]
    }
   ],
   "source": [
    "#Lets look the data\n",
    "for i in twitter.text.head(10):\n",
    "    print(i)\n",
    "#It looks like there are many situations where text doesn't seem related to disasters. We should have considered this before in our ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "related                   0.975266\n",
       "request                   0.000419\n",
       "offer                     0.000000\n",
       "aid_related               0.005450\n",
       "medical_help              0.000000\n",
       "medical_products          0.000000\n",
       "search_and_rescue         0.000000\n",
       "security                  0.000000\n",
       "military                  0.000279\n",
       "child_alone               0.000000\n",
       "water                     0.000000\n",
       "food                      0.000140\n",
       "shelter                   0.000000\n",
       "clothing                  0.000000\n",
       "money                     0.000000\n",
       "missing_people            0.000000\n",
       "refugees                  0.000000\n",
       "death                     0.000000\n",
       "other_aid                 0.000000\n",
       "infrastructure_related    0.000000\n",
       "transport                 0.000838\n",
       "buildings                 0.000000\n",
       "electricity               0.000000\n",
       "tools                     0.000000\n",
       "hospitals                 0.000000\n",
       "shops                     0.000000\n",
       "aid_centers               0.000000\n",
       "other_infrastructure      0.000000\n",
       "weather_related           0.035634\n",
       "floods                    0.000000\n",
       "storm                     0.000838\n",
       "fire                      0.000000\n",
       "earthquake                0.000000\n",
       "cold                      0.000000\n",
       "other_weather             0.000000\n",
       "direct_report             0.000978\n",
       "dtype: float64"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict on new data\n",
    "new_predictions = pipeline.predict(twitter.text)\n",
    "teste = pd.concat([twitter.text,pd.DataFrame(new_predictions)],axis=1)\n",
    "teste.columns = teste.columns = ['text'] + list(Y.columns)\n",
    "analysis = teste.iloc[:,1:].sum() / teste.shape[0]\n",
    "analysis\n",
    "\n",
    "# As we can see, most of the data is considered related, but we have seen that this is not the case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation - One Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
